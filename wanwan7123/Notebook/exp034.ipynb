{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":373,"status":"ok","timestamp":1654997123079,"user":{"displayName":"Tomoya Yanagi","userId":"11157828680567606809"},"user_tz":-540},"id":"k3QT3hl827yR","outputId":"9566616d-93cc-4bc9-9800-ab012d931c1f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sun Jun 12 01:25:22 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["! nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iri53t0J3Ma0"},"outputs":[],"source":["import os\n","\n","class Config:\n","    AUTHOR = \"wanwan7123\"\n","\n","    NAME = \"USP-exp034-roberta-large-baseline\"\n","    MODEL_PATH = \"roberta-large\"\n","    DATASET_PATH = []\n","\n","    COMPETITION = \"us-patent-phrase-to-phrase-matching\"\n","    COLAB_PATH = \"/content/drive/MyDrive/DataAnalysis/competicion/competition_USPPPM\" \n","    DRIVE_PATH = os.path.join(COLAB_PATH, AUTHOR)\n","\n","    api_path = \"/content/drive/MyDrive/kaggle.json\"\n","\n","    seed = 42\n","    num_fold = 5\n","    trn_fold = [0, 1, 2, 3, 4]\n","    batch_size = 16\n","    n_epochs = 5\n","    max_len = 130\n","    \n","    fc_dropout = 0.1\n","    weight_decay = 0.01\n","    beta = (0.9, 0.98)\n","    lr = 1e-5\n","    num_warmup_steps_rate = 0.01\n","    clip_grad_norm = None\n","    gradient_accumulation_steps = 1\n","\n","    upload_from_colab = True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":95589,"status":"ok","timestamp":1654997228023,"user":{"displayName":"Tomoya Yanagi","userId":"11157828680567606809"},"user_tz":-540},"id":"Y3qpAE-53Teb","outputId":"bf5a5483-c86a-4139-f65e-0f0d785e89ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.9.1\n","  Downloading torch-1.9.1-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n","\u001b[K     |████████████████████████████████| 831.4 MB 5.8 kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.1) (4.2.0)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.11.0+cu113\n","    Uninstalling torch-1.11.0+cu113:\n","      Successfully uninstalled torch-1.11.0+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.9.1 which is incompatible.\n","torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.9.1 which is incompatible.\n","torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.9.1 which is incompatible.\u001b[0m\n","Successfully installed torch-1.9.1\n"]}],"source":["import os\n","import re\n","import gc\n","import sys\n","import json\n","import time\n","import shutil\n","import joblib\n","import random\n","import requests\n","import warnings\n","warnings.filterwarnings('ignore')\n","from ast import literal_eval\n","from tqdm.auto import tqdm\n","from pathlib import Path\n","from glob import glob\n","\n","import numpy as np\n","import pandas as pd\n","import scipy \n","import itertools\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import (\n","    StratifiedKFold, \n","    KFold, \n","    GroupKFold,\n","    StratifiedGroupKFold\n",")\n","\n","! pip install torch==1.9.1\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from torch.cuda.amp import autocast, GradScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5zS5FvS83UY_"},"outputs":[],"source":["def setup(cfg):\n","    cfg.COLAB = 'google.colab' in sys.modules\n","    cfg.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    if cfg.COLAB:\n","        print('This environment is Google Colab')\n","\n","        # mount\n","        from google.colab import drive\n","        if not os.path.isdir('/content/drive'):\n","            drive.mount('/content/drive') \n","\n","        # pip install\n","        ! pip install transformers==4.16.2\n","        ! pip install tokenizers==0.11.6\n","        ! pip install transformers[sentencepiece]\n","\n","        # use kaggle api (need kaggle token)\n","        f = open(cfg.api_path, 'r')\n","        json_data = json.load(f) \n","        os.environ['KAGGLE_USERNAME'] = json_data['username']\n","        os.environ['KAGGLE_KEY'] = json_data['key']\n","\n","        # set dirs\n","        cfg.DRIVE = cfg.DRIVE_PATH\n","        cfg.EXP = (cfg.NAME if cfg.NAME is not None \n","            else requests.get('http://172.28.0.2:9000/api/sessions').json()[0]['name'][:-6]\n","        )\n","        cfg.INPUT = os.path.join(cfg.DRIVE, 'Input')\n","        cfg.OUTPUT = os.path.join(cfg.DRIVE, 'Output')\n","        cfg.SUBMISSION = os.path.join(cfg.DRIVE, 'Submission')\n","        cfg.DATASET = os.path.join(cfg.DRIVE, 'Dataset')\n","\n","        cfg.OUTPUT_EXP = os.path.join(cfg.OUTPUT, cfg.EXP) \n","        cfg.EXP_MODEL = os.path.join(cfg.OUTPUT_EXP, 'model')\n","        cfg.EXP_FIG = os.path.join(cfg.OUTPUT_EXP, 'fig')\n","        cfg.EXP_PREDS = os.path.join(cfg.OUTPUT_EXP, 'preds')\n","\n","        # make dirs\n","        for d in [cfg.INPUT, cfg.SUBMISSION, cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS]:\n","            os.makedirs(d, exist_ok=True)\n","        \n","        if not os.path.isfile(os.path.join(cfg.INPUT, 'train.csv')):\n","            # load dataset\n","            ! pip install --upgrade --force-reinstall --no-deps kaggle\n","            ! kaggle competitions download -c $cfg.COMPETITION -p $cfg.INPUT\n","            filepath = os.path.join(cfg.INPUT,cfg.COMPETITION+'.zip')\n","            ! unzip -d $cfg.INPUT $filepath\n","            \n","        \n","        for path in cfg.DATASET_PATH:\n","            datasetpath = os.path.join(cfg.DATASET,  path.split('/')[1])\n","            if not os.path.exists(datasetpath):\n","                os.makedirs(datasetpath, exist_ok=True)\n","                ! kaggle datasets download $path -p $datasetpath\n","                filepath = os.path.join(datasetpath, path.split(\"/\")[1]+'.zip')\n","                ! unzip -d $datasetpath $filepath\n","\n","    else:\n","        print('This environment is Kaggle Kernel')\n","\n","        # set dirs\n","        cfg.INPUT = f'../input/{cfg.COMPETITION}'\n","        cfg.EXP = cfg.NAME\n","        cfg.OUTPUT_EXP = cfg.NAME\n","        cfg.SUBMISSION = './'\n","        cfg.DATASET = '../input/'\n","        \n","        cfg.EXP_MODEL = os.path.join(cfg.EXP, 'model')\n","        cfg.EXP_FIG = os.path.join(cfg.EXP, 'fig')\n","        cfg.EXP_PREDS = os.path.join(cfg.EXP, 'preds')\n","\n","        # make dirs\n","        for d in [cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS]:\n","            os.makedirs(d, exist_ok=True)\n","    return cfg\n","\n","\n","def dataset_create_new(dataset_name, upload_dir):\n","    dataset_metadata = {}\n","    dataset_metadata['id'] = f'{os.environ[\"KAGGLE_USERNAME\"]}/{dataset_name}'\n","    dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n","    dataset_metadata['title'] = dataset_name\n","    with open(os.path.join(upload_dir, 'dataset-metadata.json'), 'w') as f:\n","        json.dump(dataset_metadata, f, indent=4)\n","    api = KaggleApi()\n","    api.authenticate()\n","    api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode='tar')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQS0vaZd8A8-"},"outputs":[],"source":["# =====================\n","# Utils\n","# =====================\n","# Seed\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","# KFold\n","def get_kfold(train, n_splits, seed):\n","    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n","    generator = kf.split(train)\n","    fold_series = []\n","    for fold, (idx_train, idx_valid) in enumerate(generator):\n","        fold_series.append(pd.Series(fold, index=idx_valid))\n","    fold_series = pd.concat(fold_series).sort_index()\n","    return fold_series\n","\n","def get_stratifiedkfold(train, target_col, n_splits, seed):\n","    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n","    generator = kf.split(train, train[target_col])\n","    fold_series = []\n","    for fold, (idx_train, idx_valid) in enumerate(generator):\n","        fold_series.append(pd.Series(fold, index=idx_valid))\n","    fold_series = pd.concat(fold_series).sort_index()\n","    return fold_series\n","\n","def get_groupkfold(train, target_col, group_col, n_splits):\n","    kf = GroupKFold(n_splits=n_splits)\n","    generator = kf.split(train, train[target_col], train[group_col])\n","    fold_series = []\n","    for fold, (idx_train, idx_valid) in enumerate(generator):\n","        fold_series.append(pd.Series(fold, index=idx_valid))\n","    fold_series = pd.concat(fold_series).sort_index()\n","    return fold_series\n","\n","def get_groupstratifiedkfold(train, target_col, group_col, n_splits, seed):\n","    kf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n","    train['score_map'] = train[target_col].map({0.00: 0, 0.25: 1, 0.50: 2, 0.75: 3, 1.00: 4})\n","    generator = kf.split(train, train['score_map'], train[group_col])\n","    fold_series = []\n","    for fold, (idx_train, idx_valid) in enumerate(generator):\n","        fold_series.append(pd.Series(fold, index=idx_valid))\n","    fold_series = pd.concat(fold_series).sort_index()\n","    return fold_series"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9c3uiSU3mDy"},"outputs":[],"source":["# =====================\n","# Dataset, Model\n","# =====================\n","\n","def get_cpc_texts():\n","    '''\n","    contextを説明文に変える\n","    '''\n","    contexts = []\n","    pattern = '[A-Z]\\d+'\n","    for file_name in os.listdir(os.path.join(cfg.INPUT, 'CPCSchemeXML202105')):\n","        result = re.findall(pattern, file_name)\n","        if result:\n","            contexts.append(result)\n","    contexts = sorted(set(sum(contexts, [])))\n","    results = {}\n","    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n","        with open(os.path.join(cfg.INPUT, f'CPCTitleList202202/cpc-section-{cpc}_20220201.txt')) as f:\n","            s = f.read()\n","        pattern = f'{cpc}\\t\\t.+'\n","        result = re.findall(pattern, s)\n","        pattern = \"^\"+pattern[:-2]\n","        cpc_result = re.sub(pattern, \"\", result[0])\n","        for context in [c for c in contexts if c[0] == cpc]:\n","            pattern = f'{context}\\t\\t.+'\n","            result = re.findall(pattern, s)\n","            pattern = \"^\"+pattern[:-2]\n","            results[context] = cpc_result + \". \" + re.sub(pattern, \"\", result[0])\n","    return results\n","\n","def processing_features(df):\n","    cpc_texts = get_cpc_texts()\n","    torch.save(cpc_texts, cfg.EXP_FIG+\"cpc_texts.pth\")\n","    df['context_text'] = df['context'].map(cpc_texts)\n","    # 区切りを入れて全部くっつける\n","    df['text'] = df['anchor'] + '[SEP]' + df['target'] + '[SEP]' + df['context_text']\n","    return df\n","\n","# dataset\n","class TrainDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.texts = df['text'].to_numpy()\n","        self.labels = df['score'].to_numpy()\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, index):\n","        inputs = self.prepare_input(self.cfg, self.texts[index])\n","        label = torch.tensor(self.labels[index], dtype=torch.half)\n","        return inputs, label\n","\n","    @staticmethod\n","    def prepare_input(cfg, text):\n","        inputs = cfg.tokenizer(text, \n","                               add_special_tokens=True,\n","                               max_length=cfg.max_len,\n","                               padding=\"max_length\",\n","                               truncation=True,\n","                               return_offsets_mapping=False)\n","        for k, v in inputs.items():\n","            inputs[k] = torch.tensor(v, dtype=torch.long)\n","        return inputs\n","\n","def collatte(inputs, labels=None):\n","    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n","    if not labels is None:\n","        inputs = {\n","            \"input_ids\" : inputs['input_ids'][:,:mask_len],\n","            \"attention_mask\" : inputs['attention_mask'][:,:mask_len],\n","        }\n","        labels =  labels[:,:mask_len]\n","        return inputs, labels, mask_len\n","                \n","    else:\n","        inputs = {\n","            \"input_ids\" : inputs['input_ids'][:,:mask_len],\n","            \"attention_mask\" : inputs['attention_mask'][:,:mask_len],\n","        }\n","        return inputs, mask_len\n","\n","class CustomModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.config = AutoConfig.from_pretrained(\n","            cfg.MODEL_PATH,\n","            output_hidden_states=True\n","        )\n","        self.backbone = AutoModel.from_pretrained(\n","            cfg.MODEL_PATH,\n","            config=self.config\n","        )\n","        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n","        self.linear1 = nn.Sequential(\n","            nn.Linear(self.config.hidden_size, 1024),\n","            nn.SELU(),\n","            nn.Linear(1024, 1024),\n","            nn.SELU(),\n","            nn.Linear(1024, 1)\n","        )\n","\n","    def forward(self, inputs):\n","        outputs = self.backbone(**inputs)[\"last_hidden_state\"]\n","        outputs = outputs[:, 0, :]\n","        outputs = self.linear1(self.fc_dropout(outputs))\n","        # 平坦化して返す\n","        return outputs.flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l-kDMGg-8i-I"},"outputs":[],"source":["def training(cfg, train):\n","    # =====================\n","    # Training\n","    # =====================\n","    set_seed(cfg.seed)\n","    oof_pred = np.zeros(len(train), dtype=np.float32)\n","    for fold in cfg.trn_fold:\n","        # dataset, dataloader\n","        train_df = train.loc[cfg.folds!=fold]\n","        valid_df = train.loc[cfg.folds==fold]\n","        train_idx = list(train_df.index)\n","        valid_idx = list(valid_df.index)\n","\n","        train_dataset = TrainDataset(cfg, train_df)\n","        valid_dataset = TrainDataset(cfg, valid_df)\n","        train_loader = DataLoader(\n","            dataset=train_dataset, \n","            batch_size=cfg.batch_size, \n","            shuffle=True,\n","            pin_memory=True,\n","            drop_last=True\n","        )\n","        valid_loader = DataLoader(\n","            dataset=valid_dataset,\n","            batch_size=cfg.batch_size,\n","            shuffle=False,\n","            pin_memory=True,\n","            drop_last=False\n","        )\n","\n","        # model\n","        model = CustomModel(cfg)\n","        torch.save(model.config, cfg.EXP_MODEL+'config.pth')\n","        model = model.to(cfg.device)\n","\n","        # optimizer, scheduler\n","        param_optimizer = list(model.named_parameters())\n","        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","        optimizer_grouped_parameters = [\n","            {\n","                'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n","                'weight_decay': cfg.weight_decay\n","            },\n","            {\n","                'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n","                'weight_decay': 0.0\n","            }\n","        ]\n","        optimizer = AdamW(\n","            optimizer_grouped_parameters,\n","            lr=cfg.lr,\n","            betas=cfg.beta,\n","            weight_decay=cfg.weight_decay,\n","        )\n","        num_train_optimization_steps = int(\n","            len(train_loader) * cfg.n_epochs // cfg.gradient_accumulation_steps\n","        )\n","        num_warmup_steps = int(num_train_optimization_steps * cfg.num_warmup_steps_rate)\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_train_optimization_steps\n","        )\n","\n","        # model-training\n","        criterion = nn.MSELoss()\n","        best_val_score = -1\n","        \n","        for epoch in range(cfg.n_epochs):\n","            # training\n","            print(f\"# ============ start epoch:{epoch} ============== #\")\n","            model.train() \n","            val_losses_batch = []\n","            scaler = GradScaler()\n","            with tqdm(train_loader, total=len(train_loader)) as pbar:\n","                for step, (inputs, labels) in enumerate(pbar):\n","                    for k, v in inputs.items():\n","                        inputs[k] = v.to(cfg.device)\n","                    labels = labels.to(cfg.device)\n","\n","                    optimizer.zero_grad()\n","                    with autocast():\n","                        output = model(inputs)\n","                    loss = criterion(output, labels)\n","                    pbar.set_postfix({\n","                        'loss': loss.item(),\n","                        'lr': scheduler.get_lr()[0]\n","                    })\n","\n","                    if cfg.gradient_accumulation_steps > 1:\n","                        loss = loss / cfg.gradient_accumulation_steps\n","\n","                    scaler.scale(loss).backward()\n","                    if cfg.clip_grad_norm is not None:\n","                        torch.nn.utils.clip_grad_norm_(\n","                            model.parameters(), \n","                            cfg.clip_grad_norm\n","                        )\n","                    if (step+1) % cfg.gradient_accumulation_steps == 0:\n","                        scaler.step(optimizer)\n","                        scaler.update()\n","                        scheduler.step()\n","\n","            # evaluating\n","            val_preds = []\n","            val_losses = []\n","            val_nums = []\n","            model.eval()\n","            with torch.no_grad():\n","                with tqdm(valid_loader, total=len(valid_loader)) as pbar:\n","                    for (inputs, labels) in pbar:\n","                        for k, v in inputs.items():\n","                            inputs[k] = v.to(cfg.device)\n","                        labels = labels.to(cfg.device)\n","                        with autocast():\n","                            output = model(inputs)\n","                        loss = criterion(output, labels.to(torch.float))\n","                        \n","                        output = output.detach().cpu().numpy()\n","                        val_preds.append(output)\n","                        val_losses.append(loss.item() * len(labels))\n","                        val_nums.append(len(labels))\n","                        pbar.set_postfix({\n","                            'val_loss': loss.item()\n","                        })\n","\n","            val_preds = np.concatenate(val_preds)\n","            val_loss = sum(val_losses) / sum(val_nums)\n","            corr_score = np.corrcoef(val_preds, valid_df['score'])[0, 1]\n","\n","            val_log = {\n","                'val_loss': val_loss,\n","                'score': corr_score,\n","            }\n","            display(val_log)\n","\n","            if best_val_score < corr_score:\n","                print(\"save model weight\")\n","                best_val_preds = val_preds\n","                best_val_score = corr_score\n","                torch.save(\n","                    model.state_dict(), \n","                    os.path.join(cfg.EXP_MODEL, f\"fold{fold}.pth\")\n","                )\n","\n","        oof_pred[valid_idx] = best_val_preds.astype(np.float32)\n","        np.save(os.path.join(cfg.EXP_PREDS, f'oof_pred_fold{fold}.npy'), best_val_preds)\n","        del model; gc.collect()\n","\n","    np.save(os.path.join(cfg.EXP_PREDS, 'oof_pred.npy'), oof_pred)\n","\n","    # =====================\n","    # scoring\n","    # =====================\n","    corr_score = np.corrcoef(oof_pred, train['score'])[0, 1]\n","    print('CV:', round(corr_score, 5))\n","    return corr_score"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["c93dd3a9ba80410f9e7063073192cfa0","ffb8a60cf3154a4ca49ff79eb2b245b9","2f4b3a81d33d4f0f8f1caaa730731fd1","b6eb0238cb254eb7a826d1d185ca92c3","5703b3679bc24edc93b1777aaedf5217","c09afe30e79a4d9a8941d5d201faff8d","0cc2558ee66f45808449c9bfc7c70b3d","050b48590a90424c98b27a9373e3f25b","a4f552f1984644d89d5e1e420f59212f","2bf5be4eabbc44aebcaa37a2a9eac65f","76a4ebac0dde45e6997e3fffe0adc26c","c5c2db61f77d40ccb2eb23feb184c715","ae71608a19bd434c8bd1bf17f4e85c3f","1f1b1faa82884f49ac934ad030d342ac","3bae6d3af125461db7590ec7259fa041","1eae7c2b41564de6b7f65f1d095d5c23","6b57986975034a9590cc9ad1fe4a41a7","905057965e764a69ab1fe6fbdd1abc19","9294affc59e44ba4b83c685a28d62684","ae67194aeda14cb8988ca735762b518e","3a02d47fe31d4938bf8bf503dfe56532","79937fa2ced046bf9fcca87ca4e0fef5","7831262434ba4d83bd42bb467d1ab1ef","4a442e75af0947108eec28be71fac66e","3ba31de3fce94476b88206703037d3e4","8c249b0e00b440dea731731599686e7e","2aa6d424256b4b7685d711d0ef989a5b","0035831f0a354c6e9be3e335f0b12fdf","00e3bc9aa85c45ef8473d2e591662b2e","bcbb6b0bf0c44c0a85c5d9713001a27c","827d4dfafe5f4ceba7de362acd2406bd","7eb952ac5f864fad98ddb8efd59fe5cf","d98c61c78b6f42208ec923a06e268860","8fabe787ed704feab698208d809056e7","b64635c644584250b0371127fe4a559e","4ed0da612337413095f72ae4e9758d9f","779d186435ec401dbc63933b972e901b","8f45af12286c4aeeaf6662320dd93877","beb3b8c1acac4bd9b9b7e5659f7ed2e1","291387d2762244eeb19daf1de7b7f1df","7b374a6b93654548a717197a823b7cd4","2eb04c0b640d45bfb4bd7ee7f461d766","9fb12517dddc4b40b5d2a26395de0b9c","42dfe838e73c4a15a6f169787820ea47","30cac54f7627485cb1cdd20b7fe4dbd7","c142f831109f4fda82cdc4b667ad1df0","03fd94dd324949ffb820936eb8889561","613fbe1c2bb344b3bfb3f97a804b21f9","e3f190516c9b4498a8adb3fa4d871e87","b95648bc29434625b1e7f8231029b6d6","c522e54913384e2a90f268d945fca35a","646365b8d7714bbcaa0023a5a19ea978","c8e9ec4cc0ff49efaa5699839c54bb80","7b421d4950e64cafa434f65548458afd","0edb1924331e473fb402e9f9cd31d69d","d09c6dd26548461199d3541a84249399","656f3baaa0304998a0f307ef9d639c8f","037c8350939c487e93692faf25bcb4fa","779968555432499991a88fb08f9d7dd4","ad2f5120899f4e00b0532228aee3df94","a97c60b7776947e98de91bae06dcc763","fb144c5453714f5492f227939089691c","f012fec8eb5b42a9bb2384b6fb8f4609","f78f418ffac445f78a8fd11e0032e341","ed761a05b5c44d7ab0a7d9442d2f5bbe","df5069288b664607b6eb120a9daeb883","725da665fb154f4989954c11c2481ba0","72dd4eb7999f4d3e843e873241ad9af5","c6b1ee8b151d4bc3bebeb504d8bc3567","e9d2548175c240229d2f6c101bab6cf1","df226997250c4eedb8dd743649719861","79502f2b013240a29b01b4f236e63ecd","8f7e791268ed46f3ade22ee4015ec537","0c27abaac6cb4ce9b6194c959be00060","d2e2dbbfe3934f428a87ffb41cd40537","634973372cd34fb7ae397c0d8154c450","b65c7b0ef160493d87d893c9c1724412","6d7d6ddfebb54e62a67fd4b4a8c1f84e","670bd4befaef42809d7c4d0b78ddf4a1","3594b1c052ad4bd89462a3dee738c02d","1c58dd326b564aca90d9865cfc3f64b6","1e865fb3cb604c4992dd6cb6e63a259a","b64062471a204f0e9daa36f8fc9fe3aa","9cd754c30c324df7b0fed1d2d800e158","02388c7ca2dc4f0ead98bbb8cb05af09","23f278e99d9f48a489c785ebb5ba8dbd","4878bb3ce2f042d28f3a901b8f8aa720","732c0630c1cd44a1849f71712b30e278","dc753f385a074f3f9d747d24df749467","6afd13522357480498ba66a21e3e52df","8d14c1dba14449dcb9ff24afba445ee0","c159f81c30964ab2a3ae6dad7b5b0883","ebbade96381745ec9de2047a727ee82a","458e1382f0da4a36a2beef04e3f2e280","4a529bf791d043b78e27f49248e12f31","73c3aa4dd0c147e09895b81015688dc6","63ff53ee841043aab36b2c94eff41a88","03ea810543c0476b905950d55cc38dc7","8b44a032bacd483e9fa9992cf518e8df","b1793eea37bf4edd86db6071dfbe9548","7deb5537a51c480a951599c700a53aed","82759c13bf7342ef8c87c3dda7ea2ab9","859898212f5d494d8d9398940c4980fb","626fdadeee764617a1b89334b96f8451","60da99f3bb15489e9789f12ee333815f"]},"id":"gSrrvDWVpxN_","outputId":"07794b82-9c88-41a8-c629-fd4b74658386","executionInfo":{"status":"ok","timestamp":1655007268603,"user_tz":-540,"elapsed":560841,"user":{"displayName":"Tomoya Yanagi","userId":"11157828680567606809"}}},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["This environment is Google Colab\n","Mounted at /content/drive\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==4.16.2\n","  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 14.7 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2019.12.20)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (3.7.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.11.4)\n","Collecting tokenizers!=0.11.3,>=0.10.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 74.9 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 72.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (1.21.6)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 79.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.64.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 6.3 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.16.2) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.16.2) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2022.5.18.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.1.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=ec26a4d88c4e0855bc8fbaf2b75c0b4aa41e290ee27f1f383695193286ea3fa1\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.16.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tokenizers==0.11.6\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 14.4 MB/s \n","\u001b[?25hInstalling collected packages: tokenizers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.12.1\n","    Uninstalling tokenizers-0.12.1:\n","      Successfully uninstalled tokenizers-0.12.1\n","Successfully installed tokenizers-0.11.6\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.7/dist-packages (4.16.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.7.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.0.53)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.11.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.64.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.11.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (1.21.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.7.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n","Collecting sentencepiece!=0.1.92,>=0.1.91\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 14.6 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[sentencepiece]) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.8.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers[sentencepiece]) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2022.5.18.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (7.1.2)\n","Installing collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","env: TOKENIZERS_PARALLELISM=true\n","tokenizers.__version__: 0.11.6\n","transformers.__version__: 4.16.2\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c93dd3a9ba80410f9e7063073192cfa0","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/482 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ffb8a60cf3154a4ca49ff79eb2b245b9","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f4b3a81d33d4f0f8f1caaa730731fd1","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6eb0238cb254eb7a826d1d185ca92c3","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5703b3679bc24edc93b1777aaedf5217","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["# ============ start epoch:0 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c09afe30e79a4d9a8941d5d201faff8d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1821 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0cc2558ee66f45808449c9bfc7c70b3d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/459 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.769559596327499, 'val_loss': 0.030605447068424423}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:1 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"050b48590a90424c98b27a9373e3f25b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1821 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a4f552f1984644d89d5e1e420f59212f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/459 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.7821989792732019, 'val_loss': 0.028055903621430386}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:2 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2bf5be4eabbc44aebcaa37a2a9eac65f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1821 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"76a4ebac0dde45e6997e3fffe0adc26c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/459 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.7894780665773771, 'val_loss': 0.026094020167551073}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:3 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c5c2db61f77d40ccb2eb23feb184c715","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1821 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae71608a19bd434c8bd1bf17f4e85c3f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/459 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.792461523933601, 'val_loss': 0.02605103717378831}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:4 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f1b1faa82884f49ac934ad030d342ac","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1821 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3bae6d3af125461db7590ec7259fa041","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/459 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.7912845677771424, 'val_loss': 0.02652398984775634}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["# ============ start epoch:0 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1eae7c2b41564de6b7f65f1d095d5c23","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1845 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b57986975034a9590cc9ad1fe4a41a7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/435 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.7616320242933865, 'val_loss': 0.030608898218884753}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:1 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"905057965e764a69ab1fe6fbdd1abc19","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1845 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9294affc59e44ba4b83c685a28d62684","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/435 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.784985354654987, 'val_loss': 0.025357093179800942}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:2 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae67194aeda14cb8988ca735762b518e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1845 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a02d47fe31d4938bf8bf503dfe56532","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/435 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.7854172516300337, 'val_loss': 0.028377148620328342}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:3 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"79937fa2ced046bf9fcca87ca4e0fef5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1845 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7831262434ba4d83bd42bb467d1ab1ef","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/435 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.7934759737091748, 'val_loss': 0.025750393900803047}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:4 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a442e75af0947108eec28be71fac66e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1845 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3ba31de3fce94476b88206703037d3e4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/435 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.7917302097584495, 'val_loss': 0.02578273248517466}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["# ============ start epoch:0 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c249b0e00b440dea731731599686e7e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1825 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2aa6d424256b4b7685d711d0ef989a5b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/455 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.7700409724246777, 'val_loss': 0.029890082439676946}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:1 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0035831f0a354c6e9be3e335f0b12fdf","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1825 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"00e3bc9aa85c45ef8473d2e591662b2e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/455 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.7855980888602964, 'val_loss': 0.02598063769788595}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:2 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"291387d2762244eeb19daf1de7b7f1df","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1825 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c522e54913384e2a90f268d945fca35a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/455 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.7985637136959668, 'val_loss': 0.024284662486706864}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:3 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"646365b8d7714bbcaa0023a5a19ea978","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1825 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c8e9ec4cc0ff49efaa5699839c54bb80","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/455 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.7961392092540248, 'val_loss': 0.025323821343799197}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["# ============ start epoch:4 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b421d4950e64cafa434f65548458afd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1825 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0edb1924331e473fb402e9f9cd31d69d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/455 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.7981529263829689, 'val_loss': 0.025234383957317322}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["# ============ start epoch:0 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d09c6dd26548461199d3541a84249399","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1817 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"656f3baaa0304998a0f307ef9d639c8f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/463 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.7533591307853815, 'val_loss': 0.03072827120260832}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:1 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"037c8350939c487e93692faf25bcb4fa","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1817 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"779968555432499991a88fb08f9d7dd4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/463 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.778617589096968, 'val_loss': 0.025862552512702443}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:2 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad2f5120899f4e00b0532228aee3df94","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1817 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a97c60b7776947e98de91bae06dcc763","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/463 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.7862024897061338, 'val_loss': 0.026365331806980575}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:3 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb144c5453714f5492f227939089691c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1817 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f012fec8eb5b42a9bb2384b6fb8f4609","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/463 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.7896778812892141, 'val_loss': 0.02527126247144592}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:4 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f78f418ffac445f78a8fd11e0032e341","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1817 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ed761a05b5c44d7ab0a7d9442d2f5bbe","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/463 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.7889550959361072, 'val_loss': 0.025760989523024386}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["# ============ start epoch:0 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df5069288b664607b6eb120a9daeb883","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1809 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"725da665fb154f4989954c11c2481ba0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/471 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.7875697825865942, 'val_loss': 0.028715917298442535}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:1 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72dd4eb7999f4d3e843e873241ad9af5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1809 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c6b1ee8b151d4bc3bebeb504d8bc3567","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/471 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.797351624692798, 'val_loss': 0.029914408794949342}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:2 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9d2548175c240229d2f6c101bab6cf1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1809 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df226997250c4eedb8dd743649719861","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/471 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.8061881726496603, 'val_loss': 0.02894664620500353}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:3 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1e865fb3cb604c4992dd6cb6e63a259a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1809 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ebbade96381745ec9de2047a727ee82a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/471 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'score': 0.8076807804216127, 'val_loss': 0.02559165653076017}"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:4 ============== #\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"458e1382f0da4a36a2beef04e3f2e280","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1809 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/471 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a529bf791d043b78e27f49248e12f31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["{'score': 0.8102526768270399, 'val_loss': 0.02517148098217176}"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["save model weight\n","CV: 0.79602\n","Starting upload for file model.tar\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6.66G/6.66G [02:54<00:00, 41.0MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Upload successful: model.tar (7GB)\n","Starting upload for file fig.tar\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10.0k/10.0k [00:02<00:00, 3.80kB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Upload successful: fig.tar (10KB)\n","Starting upload for file preds.tar\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 500k/500k [00:02<00:00, 186kB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Upload successful: preds.tar (500KB)\n","Starting upload for file figcpc_texts.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13.9k/13.9k [00:02<00:00, 4.81kB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Upload successful: figcpc_texts.pth (14KB)\n","Starting upload for file tokenizer.tar\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3.22M/3.22M [00:02<00:00, 1.18MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Upload successful: tokenizer.tar (3MB)\n","Starting upload for file modelconfig.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2.17k/2.17k [00:01<00:00, 1.28kB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Upload successful: modelconfig.pth (2KB)\n"]}],"source":["# =====================\n","# Main\n","# =====================\n","\n","# setup\n","cfg = setup(Config)\n","\n","import transformers\n","from transformers import AutoConfig, AutoModel, AutoTokenizer\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","import tokenizers\n","import sentencepiece\n","%env TOKENIZERS_PARALLELISM=true\n","print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n","print(f\"transformers.__version__: {transformers.__version__}\")\n","\n","# main\n","train = pd.read_csv(os.path.join(cfg.INPUT, 'train.csv'))\n","test = pd.read_csv(os.path.join(cfg.INPUT, 'test.csv'))\n","sub = pd.read_csv(os.path.join(cfg.INPUT, 'sample_submission.csv'))\n","\n","train = processing_features(train)\n","test = processing_features(test)\n","\n","cfg.tokenizer = AutoTokenizer.from_pretrained(cfg.MODEL_PATH)\n","cfg.tokenizer.save_pretrained(os.path.join(cfg.OUTPUT_EXP, 'tokenizer'))\n","cfg.folds = get_groupstratifiedkfold(train, 'score', 'anchor', cfg.num_fold, cfg.seed)\n","cfg.folds.to_csv(os.path.join(cfg.EXP_PREDS, 'folds.csv'))\n","score = training(cfg, train)\n","\n","if cfg.upload_from_colab and cfg.COLAB:\n","    from kaggle.api.kaggle_api_extended import KaggleApi\n","    dataset_create_new(dataset_name=Config.EXP, upload_dir=Config.OUTPUT_EXP)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYumfMTa81vp"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"exp034-roberta-large.ipynb","provenance":[],"mount_file_id":"1UYXX3Wlqu3EOee9sQo_J0qP_XipGi2GY","authorship_tag":"ABX9TyPghspmir61U7uuAj05ejkR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00e3bc9aa85c45ef8473d2e591662b2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bcbb6b0bf0c44c0a85c5d9713001a27c","IPY_MODEL_827d4dfafe5f4ceba7de362acd2406bd","IPY_MODEL_7eb952ac5f864fad98ddb8efd59fe5cf"],"layout":"IPY_MODEL_d98c61c78b6f42208ec923a06e268860"}},"03fd94dd324949ffb820936eb8889561":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"291387d2762244eeb19daf1de7b7f1df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7b374a6b93654548a717197a823b7cd4","IPY_MODEL_2eb04c0b640d45bfb4bd7ee7f461d766","IPY_MODEL_9fb12517dddc4b40b5d2a26395de0b9c"],"layout":"IPY_MODEL_42dfe838e73c4a15a6f169787820ea47"}},"2eb04c0b640d45bfb4bd7ee7f461d766":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_03fd94dd324949ffb820936eb8889561","max":1825,"min":0,"orientation":"horizontal","style":"IPY_MODEL_613fbe1c2bb344b3bfb3f97a804b21f9","value":115}},"30cac54f7627485cb1cdd20b7fe4dbd7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42dfe838e73c4a15a6f169787820ea47":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ed0da612337413095f72ae4e9758d9f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"613fbe1c2bb344b3bfb3f97a804b21f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"779d186435ec401dbc63933b972e901b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7b374a6b93654548a717197a823b7cd4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30cac54f7627485cb1cdd20b7fe4dbd7","placeholder":"​","style":"IPY_MODEL_c142f831109f4fda82cdc4b667ad1df0","value":"  6%"}},"7eb952ac5f864fad98ddb8efd59fe5cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f45af12286c4aeeaf6662320dd93877","placeholder":"​","style":"IPY_MODEL_beb3b8c1acac4bd9b9b7e5659f7ed2e1","value":" 455/455 [00:25&lt;00:00, 17.93it/s, val_loss=0.0439]"}},"827d4dfafe5f4ceba7de362acd2406bd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ed0da612337413095f72ae4e9758d9f","max":455,"min":0,"orientation":"horizontal","style":"IPY_MODEL_779d186435ec401dbc63933b972e901b","value":455}},"8f45af12286c4aeeaf6662320dd93877":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fabe787ed704feab698208d809056e7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fb12517dddc4b40b5d2a26395de0b9c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3f190516c9b4498a8adb3fa4d871e87","placeholder":"​","style":"IPY_MODEL_b95648bc29434625b1e7f8231029b6d6","value":" 115/1825 [00:22&lt;05:40,  5.02it/s, loss=0.0322, lr=5.93e-6]"}},"b64635c644584250b0371127fe4a559e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b95648bc29434625b1e7f8231029b6d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bcbb6b0bf0c44c0a85c5d9713001a27c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fabe787ed704feab698208d809056e7","placeholder":"​","style":"IPY_MODEL_b64635c644584250b0371127fe4a559e","value":"100%"}},"beb3b8c1acac4bd9b9b7e5659f7ed2e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c142f831109f4fda82cdc4b667ad1df0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d98c61c78b6f42208ec923a06e268860":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3f190516c9b4498a8adb3fa4d871e87":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df226997250c4eedb8dd743649719861":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_79502f2b013240a29b01b4f236e63ecd","IPY_MODEL_8f7e791268ed46f3ade22ee4015ec537","IPY_MODEL_0c27abaac6cb4ce9b6194c959be00060"],"layout":"IPY_MODEL_d2e2dbbfe3934f428a87ffb41cd40537"}},"79502f2b013240a29b01b4f236e63ecd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_634973372cd34fb7ae397c0d8154c450","placeholder":"​","style":"IPY_MODEL_b65c7b0ef160493d87d893c9c1724412","value":"100%"}},"8f7e791268ed46f3ade22ee4015ec537":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d7d6ddfebb54e62a67fd4b4a8c1f84e","max":471,"min":0,"orientation":"horizontal","style":"IPY_MODEL_670bd4befaef42809d7c4d0b78ddf4a1","value":471}},"0c27abaac6cb4ce9b6194c959be00060":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3594b1c052ad4bd89462a3dee738c02d","placeholder":"​","style":"IPY_MODEL_1c58dd326b564aca90d9865cfc3f64b6","value":" 471/471 [00:26&lt;00:00, 17.72it/s, val_loss=0.0132]"}},"d2e2dbbfe3934f428a87ffb41cd40537":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"634973372cd34fb7ae397c0d8154c450":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b65c7b0ef160493d87d893c9c1724412":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d7d6ddfebb54e62a67fd4b4a8c1f84e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"670bd4befaef42809d7c4d0b78ddf4a1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3594b1c052ad4bd89462a3dee738c02d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c58dd326b564aca90d9865cfc3f64b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e865fb3cb604c4992dd6cb6e63a259a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b64062471a204f0e9daa36f8fc9fe3aa","IPY_MODEL_9cd754c30c324df7b0fed1d2d800e158","IPY_MODEL_02388c7ca2dc4f0ead98bbb8cb05af09"],"layout":"IPY_MODEL_23f278e99d9f48a489c785ebb5ba8dbd"}},"b64062471a204f0e9daa36f8fc9fe3aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4878bb3ce2f042d28f3a901b8f8aa720","placeholder":"​","style":"IPY_MODEL_732c0630c1cd44a1849f71712b30e278","value":"  2%"}},"9cd754c30c324df7b0fed1d2d800e158":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc753f385a074f3f9d747d24df749467","max":1809,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6afd13522357480498ba66a21e3e52df","value":39}},"02388c7ca2dc4f0ead98bbb8cb05af09":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d14c1dba14449dcb9ff24afba445ee0","placeholder":"​","style":"IPY_MODEL_c159f81c30964ab2a3ae6dad7b5b0883","value":" 39/1809 [00:07&lt;05:48,  5.09it/s, loss=0.0122, lr=4e-6]"}},"23f278e99d9f48a489c785ebb5ba8dbd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4878bb3ce2f042d28f3a901b8f8aa720":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"732c0630c1cd44a1849f71712b30e278":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc753f385a074f3f9d747d24df749467":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6afd13522357480498ba66a21e3e52df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8d14c1dba14449dcb9ff24afba445ee0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c159f81c30964ab2a3ae6dad7b5b0883":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a529bf791d043b78e27f49248e12f31":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_73c3aa4dd0c147e09895b81015688dc6","IPY_MODEL_63ff53ee841043aab36b2c94eff41a88","IPY_MODEL_03ea810543c0476b905950d55cc38dc7"],"layout":"IPY_MODEL_8b44a032bacd483e9fa9992cf518e8df"}},"73c3aa4dd0c147e09895b81015688dc6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1793eea37bf4edd86db6071dfbe9548","placeholder":"​","style":"IPY_MODEL_7deb5537a51c480a951599c700a53aed","value":"100%"}},"63ff53ee841043aab36b2c94eff41a88":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_82759c13bf7342ef8c87c3dda7ea2ab9","max":471,"min":0,"orientation":"horizontal","style":"IPY_MODEL_859898212f5d494d8d9398940c4980fb","value":471}},"03ea810543c0476b905950d55cc38dc7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_626fdadeee764617a1b89334b96f8451","placeholder":"​","style":"IPY_MODEL_60da99f3bb15489e9789f12ee333815f","value":" 471/471 [00:26&lt;00:00, 18.08it/s, val_loss=0.0131]"}},"8b44a032bacd483e9fa9992cf518e8df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1793eea37bf4edd86db6071dfbe9548":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7deb5537a51c480a951599c700a53aed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"82759c13bf7342ef8c87c3dda7ea2ab9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"859898212f5d494d8d9398940c4980fb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"626fdadeee764617a1b89334b96f8451":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60da99f3bb15489e9789f12ee333815f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}